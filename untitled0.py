# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fkT-Lqs8Xkodcy0FOj2vSDqvdkHbfgDj
"""

# Install necessary libraries (if not already installed)
!pip install tensorflow
!pip install numpy
!pip install matplotlib
!pip install scikit-learn

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import os
from glob import glob
import cv2

# Mount Google Drive to access the dataset if it's stored there
from google.colab import drive
drive.mount('/content/drive')

import os

# Define the path to your dataset
dataset_path = '/content/drive/MyDrive/traffic_Data'  # Update this path as needed

# Check contents of the train directory
train_dir = os.path.join(dataset_path, 'train')
if os.path.exists(train_dir):
    print("Contents of train directory:")
    print(os.listdir(train_dir))
else:
    print(f"Train directory {train_dir} does not exist.")

# Check contents of the test directory
test_dir = os.path.join(dataset_path, 'test')
if os.path.exists(test_dir):
    print("Contents of test directory:")
    print(os.listdir(test_dir))
else:
    print(f"Test directory {test_dir} does not exist.")

import cv2
import numpy as np
from glob import glob

def load_train_data(train_dir, img_size=(32, 32)):
    images = []
    labels = []
    label_names = sorted(os.listdir(train_dir))
    for label in label_names:
        label_path = os.path.join(train_dir, label)
        if os.path.isdir(label_path):
            image_files = glob(os.path.join(label_path, '*.png'))
            print(f'Found {len(image_files)} images in {label_path}')
            for img_file in image_files:
                img = cv2.imread(img_file)
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = cv2.resize(img, img_size)  # Resize image to fixed size
                images.append(img)
                labels.append(int(label))
    return np.array(images), np.array(labels), label_names

# Load training data
train_images, train_labels, label_names = load_train_data(train_dir)

# Check the shapes of the loaded arrays
print(f'Train images shape: {train_images.shape}')
print(f'Train labels shape: {train_labels.shape}')
print(f'Number of classes: {len(label_names)}')

def load_test_data(test_dir, img_size=(32, 32)):
    images = []
    image_files = glob(os.path.join(test_dir, '*.png'))
    print(f'Found {len(image_files)} images in {test_dir}')
    for img_file in image_files:
        img = cv2.imread(img_file)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, img_size)  # Resize image to fixed size
        images.append(img)
    return np.array(images)

# Load test data
test_images = load_test_data(test_dir)

# Check the shapes of the loaded arrays
print(f'Test images shape: {test_images.shape}')

from tensorflow.keras.utils import to_categorical

# Normalize the images
train_images = train_images / 255.0
test_images = test_images / 255.0

# Convert labels to categorical format
num_classes = len(label_names)
train_labels = to_categorical(train_labels, num_classes)

# Check the shapes of the processed arrays
print(f'Train images shape: {train_images.shape}')
print(f'Test images shape: {test_images.shape}')
print(f'Train labels shape: {train_labels.shape}')

from sklearn.model_selection import train_test_split

# Split training data into training and validation sets
train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)

# Check the shapes of the splits
print(f'Train images shape: {train_images.shape}')
print(f'Validation images shape: {val_images.shape}')
print(f'Train labels shape: {train_labels.shape}')
print(f'Validation labels shape: {val_labels.shape}')

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Build the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Data augmentation
datagen = ImageDataGenerator(
    rotation_range=10,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=False,
    vertical_flip=False
)

# Train the model
batch_size = 16
epochs = 25

history = model.fit(datagen.flow(train_images, train_labels, batch_size=batch_size),
                    steps_per_epoch=len(train_images) // batch_size,
                    validation_data=(val_images, val_labels),
                    epochs=epochs)

# Evaluate the model on the test set
# Note: test_labels are not used here because test images are assumed to be unlabeled
test_predictions = model.predict(test_images)
print(f'Test predictions shape: {test_predictions.shape}')

# Plot training history
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Accuracy over epochs')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Loss over epochs')

plt.show()

model.save('/content/drive/My Drive/path_to_save_model/traffic_sign_model1.h5')

import cv2
import glob
import numpy as np
from tensorflow.keras.models import load_model

# Assuming you have functions for model loading (load_model) and prediction (predict)
def evaluate_model_accuracy(model, test_dir, img_size=(32, 32), ground_truth_labels=None, metric='accuracy'):
    """
    Evaluates the accuracy of a loaded model on a test dataset.

    Args:
        model: The loaded machine learning model.
        test_dir: Path to the directory containing test images.
        img_size: Target image size for resizing (default: (32, 32)).
        ground_truth_labels: Array of true labels for test images (optional,
                             depending on metric).
        metric: The evaluation metric to use (default: 'accuracy'). Valid
               options include 'accuracy', 'precision', 'recall', 'f1',
               'mse', 'mae', or 'r2'.

    Returns:
        A dictionary containing the calculated evaluation metric(s) and
        potentially additional information like confusion matrix (for classification).
    """

    # Load test images
    test_images = load_test_data(test_dir, img_size)

    # Make predictions
    predictions = predict(model, test_images)

    # Calculate chosen evaluation metric(s)
    if metric.lower() in ['accuracy', 'precision', 'recall', 'f1']:
        if ground_truth_labels is None:
            raise ValueError("Ground truth labels are required for classification metrics.")

        # Assuming ground_truth_labels are in the correct format (modify for your dataset)
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        results = {
            'accuracy': accuracy_score(ground_truth_labels, predictions),
            'precision': precision_score(ground_truth_labels, predictions, average='weighted'),
            'recall': recall_score(ground_truth_labels, predictions, average='weighted'),
            'f1': f1_score(ground_truth_labels, predictions, average='weighted')
        }
    elif metric.lower() in ['mse', 'mae']:
        # Assuming ground_truth_labels are numerical values
        from sklearn.metrics import mean_squared_error, mean_absolute_error
        if ground_truth_labels is None:
            raise ValueError("Ground truth labels are required for regression metrics.")
        results = {
            metric: eval(f"{metric}_score(ground_truth_labels, predictions)")  # Dynamic metric calculation
        }
    elif metric.lower() == 'r2':
        # Assuming ground_truth_labels are numerical values
        from sklearn.metrics import r2_score
        if ground_truth_labels is None:
            raise ValueError("Ground truth labels are required for R-squared.")
        results = {'r2': r2_score(ground_truth_labels, predictions)}
    else:
        raise ValueError(f"Invalid metric: {metric}. Valid options are 'accuracy', 'precision', "
                         f"'recall', 'f1', 'mse', 'mae', or 'r2'.")

    return results

# Example usage (assuming you have a trained model and ground truth labels)
model = load_model(model_path)
test_results = evaluate_model_accuracy(model, test_dir, ground_truth_labels=test_labels, metric='f1')
print(f"Model F1-score: {test_results['f1']:.4f}")

import os
import cv2
import numpy as np
from tensorflow.keras.models import load_model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score

def load_test_data(test_dir, img_size=(32, 32)):
    """
    Loads test images from the specified directory and resizes them to the given size.

    Args:
        test_dir: Path to the directory containing test images.
        img_size: Target image size for resizing (default: (32, 32)).

    Returns:
        A tuple of numpy arrays: (test_images, ground_truth_labels)
    """
    test_images = []
    ground_truth_labels = []

    # Debugging: Check if the test directory exists
    if not os.path.exists(test_dir):
        print(f"Test directory {test_dir} does not exist.")
        return np.array([]), np.array([])

    for label_folder in os.listdir(test_dir):
        label_path = os.path.join(test_dir, label_folder)
        if os.path.isdir(label_path):
            for image_name in os.listdir(label_path):
                image_path = os.path.join(label_path, image_name)
                img = cv2.imread(image_path)
                if img is not None:
                    img = cv2.resize(img, img_size)
                    test_images.append(img)
                    ground_truth_labels.append(int(label_folder))  # Assuming folder name is the label

    # Debugging: Check if images are loaded
    if not test_images:
        print("No images found in the test directory.")
        return np.array([]), np.array([])

    test_images = np.array(test_images, dtype=np.float32) / 255.0  # Normalize images
    ground_truth_labels = np.array(ground_truth_labels)

    # Debugging: Print the number of images loaded
    print(f"Loaded {len(test_images)} test images.")

    return test_images, ground_truth_labels

def predict(model, test_images):
    """
    Uses the model to make predictions on the test images.

    Args:
        model: The loaded machine learning model.
        test_images: Numpy array of test images.

    Returns:
        A numpy array of predicted labels.
    """
    predictions = model.predict(test_images)
    predicted_labels = np.argmax(predictions, axis=1)  # Assuming it's a classification model
    return predicted_labels

def evaluate_model_accuracy(model, test_dir, img_size=(32, 32), metric='accuracy'):
    """
    Evaluates the accuracy of a loaded model on a test dataset.

    Args:
        model: The loaded machine learning model.
        test_dir: Path to the directory containing test images.
        img_size: Target image size for resizing (default: (32, 32)).
        metric: The evaluation metric to use (default: 'accuracy'). Valid options include 'accuracy', 'precision', 'recall', 'f1', 'mse', 'mae', or 'r2'.

    Returns:
        A dictionary containing the calculated evaluation metric(s) and potentially additional information like confusion matrix (for classification).
    """

    # Load test images and labels
    test_images, ground_truth_labels = load_test_data(test_dir, img_size)

    # Debugging: Check if test data is empty
    if test_images.size == 0:
        raise ValueError("Test images are empty. Please check the test directory and try again.")

    # Make predictions
    predictions = predict(model, test_images)

    # Calculate chosen evaluation metric(s)
    if metric.lower() in ['accuracy', 'precision', 'recall', 'f1']:
        results = {
            'accuracy': accuracy_score(ground_truth_labels, predictions),
            'precision': precision_score(ground_truth_labels, predictions, average='weighted'),
            'recall': recall_score(ground_truth_labels, predictions, average='weighted'),
            'f1': f1_score(ground_truth_labels, predictions, average='weighted')
        }
    elif metric.lower() in ['mse', 'mae']:
        results = {
            'mse': mean_squared_error(ground_truth_labels, predictions),
            'mae': mean_absolute_error(ground_truth_labels, predictions)
        }
    elif metric.lower() == 'r2':
        results = {'r2': r2_score(ground_truth_labels, predictions)}
    else:
        raise ValueError(f"Invalid metric: {metric}. Valid options are 'accuracy', 'precision', 'recall', 'f1', 'mse', 'mae', or 'r2'.")

    return results

# Example usage
test_dir = "/content/drive/MyDrive/traffic_Data/train"  # Replace with your actual directory
model_path = "/content/drive/MyDrive/path_to_save_model/traffic_sign_model1.h5"  # Replace with your model path

# Load the model
model = load_model(model_path)

# Evaluate the model
try:
    test_results = evaluate_model_accuracy(model, test_dir, metric='f1')
    print(f"Model F1-score: {test_results['f1']:.4f}")
except ValueError as e:
    print(f"Error during evaluation: {e}")